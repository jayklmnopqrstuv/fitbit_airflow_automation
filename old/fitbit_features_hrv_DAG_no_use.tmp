
"""
# odp_level2_feature_store.fitbit_features_hrv
Calculate heart rate variability features for all pim members.
Features are calculated and derived from the daily bpm values.
* mean_rmssd - Estimated from average of root mean square of successive differences between normal heartbeats (daily)  
* mean_sdrr - Estimated from average of the standard deviation of all the R-R intervals (daily)  
* beat_interval_ms_mean - Average beat interval (i.e. 60000/bpm) within 24 hour (i.e. between start datetime and end datetime)
* beat_interval_ms_stddev - Standard deviation from beat interval (i.e. 60000/bpm) within 24 hour (i.e. between start datetime and end datetime)
* beat_interval_ms_min - min beat interval (i.e. 60000/bpm) within 24 hour (i.e. between start datetime and end datetime)
* beat_interval_ms_max - max beat interval (i.e. 60000/bpm) within X hour (i.e. between start datetime and end datetime)
* beat_interval_ms_rmssd - Estimated root mean square of successive differences between normal heartbeats within 24 hour (i.e. between start datetime and end datetime)
* geom_poincare_sd1 - Poincare index S1 within 24 hour (i.e. between start datetime and end datetime). The standard deviation measured along the minor axis of the Poincare ellipse is called S1, and is a measure of short term variability.
* geom_poincare_sd2 - Poincare index S1 within 24 hour (i.e. between start datetime and end datetime). The standard deviation measured along the minor axis of the Poincare ellipse is called S2, and is a measure of short term variability.
* tinn - Baseline width of the minimum square difference triangular interpolation of the highest peak of the histogram of all NN intervals within 24 hour (i.e. between start datetime and end datetime)
* rrtri - triangular index within 24 hour (i.e. between start datetime and end datetime)
* sdann - Standard deviation of the averages of NN intervals in all 5 min segments of the entire recording within 24 hour (i.e. between start datetime and end datetime)
"""
from airflow_utils.macros import common_macros
from airflow_utils import env, ip_bucket, ip_feature_store, ip_workspace

from airflow import DAG
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from airflow.contrib.operators.bigquery_table_delete_operator import BigQueryTableDeleteOperator
from airflow.contrib.operators.bigquery_to_gcs import BigQueryToCloudStorageOperator
from airflow.contrib.operators.gcs_to_bq import GoogleCloudStorageToBigQueryOperator
from airflow.providers.google.cloud.operators.gcs import GCSDeleteObjectsOperator
from airflow.providers.google.cloud.operators.gcs import GCSFileTransformOperator
from airflow.providers.google.cloud.transfers.local_to_gcs import LocalFilesystemToGCSOperator

from airflow_utils.operators import GCSFileJoinTransformOperator

from datetime import datetime, timedelta
from pathlib import Path



# Output tables
hrv_output_table    = ip_feature_store + ".fitbit_features_hrv${{ ds_nodash }}"

# Stage tables 
hrv_stage_table     = ip_workspace + ".fitbit_features_data_hrv${{ ds_nodash }}"

# Query results to csv files
gcs_dir             = "fitbit_biomarker_features/hrv/{{ ds }}"
hrv_csv             = f"{gcs_dir}/hrv_data*.csv"

# Output json files
hrv_features_json    = f"{gcs_dir}/hrv_features.json"

# Schema files
hrv_schema_json     = f"{gcs_dir}/hrv_output_schema.json"
repo_dir = Path(__file__).parent.absolute()

# Schedule 11 hours offset from UTC.
schedule_interval="0 11 * * *"

default_args = {
    "owner": "Jay Gapuz",
    "depends_on_past": False,
    "start_date": datetime(2021, 4, 25),
    "email_on_failure": env == 'prod',
    "email_on_retry": False,
    "retries": 3 if env == 'prod' else 0,
    "retry_delay": timedelta(minutes=5),
    "use_legacy_sql": False,
    "bucket": ip_bucket,
    "bucket_name": ip_bucket,
    "source_bucket": ip_bucket,
    "destination_bucket": ip_bucket,
}

with DAG("fitbit_features_hrv",
          default_args=default_args,
          schedule_interval=schedule_interval,
          user_defined_macros=common_macros,
          ) as dag:
          
    dag.doc_md = __doc__


    hrv_query = BigQueryOperator(
            task_id="hrv_query",
            sql="sql/hrv.sql",
            destination_dataset_table=hrv_stage_table,
            write_disposition="WRITE_TRUNCATE",
            time_partitioning={"field": "ds"},
            retry_delay=timedelta(minutes=30),
            )

    export_to_gcs = BigQueryToCloudStorageOperator(
            task_id="export_hrv_to_gcs",
            source_project_dataset_table=hrv_stage_table,
            destination_cloud_storage_uris=f"gs://{ip_bucket}/{hrv_csv}",
            export_format="CSV",
            )
            
    feature_calculation = GCSFileJoinTransformOperator(
            task_id="feature_calculation",
            source_objects = [hrv_csv],
            destination_objects = [hrv_features_json],
            transform_script=["python", f"{repo_dir}/fitbit_hrv_system.py", "{{ ds }}"],
            retry_delay=timedelta(minutes=5),
            )


    upload_hrv_schema = LocalFilesystemToGCSOperator(
            task_id="upload_hrv_schema",
            src=f"{repo_dir}/schema/hrv_output_schema.json",
            dst=hrv_schema_json,
            )

    hrv_features_to_gbq = GoogleCloudStorageToBigQueryOperator(
            task_id="hrv_features_to_gbq",
            source_objects=[hrv_features_json],
            source_format="NEWLINE_DELIMITED_JSON",
            destination_project_dataset_table=hrv_output_table,
            schema_object=hrv_schema_json,
            write_disposition="WRITE_TRUNCATE",
            time_partitioning={"field": "ds"},
            )
    
    cleanup_gcs = GCSDeleteObjectsOperator(
            task_id="cleanup_gcs",
            prefix=gcs_dir,
            )
    

    cleanup_gbq = BigQueryTableDeleteOperator(
            task_id="cleanup_gbq_hrv",
            deletion_dataset_table=hrv_stage_table
            )
    
    
    hrv_query >> export_to_gcs >>  feature_calculation >> hrv_features_to_gbq >> cleanup_gbq >> cleanup_gcs 
    upload_hrv_schema >> hrv_features_to_gbq